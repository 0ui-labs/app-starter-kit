# Sub-Task 12.11.3: Token Optimization für AI Adapter

## Ziel
Entwicklung von Strategien zur Reduzierung der an die LLMs gesendeten Token um 30-50%, ohne Qualitätsverlust. Direkte Kosteneinsparung durch intelligente Prompt-Komprimierung und Kontext-Management.

## Voraussetzungen
- AI Adapter mit Provider-Integration
- Token Counting Funktionalität
- Verständnis der Token-Limits verschiedener Modelle

## Umfang
- Prompt-Komprimierung und Normalisierung
- Intelligentes Kontext-Window-Management
- Conversation History Truncation
- Token Budget Management
- Dynamic Summarization

## Implementierung

### Schritt 1: Token Counter und Analyzer
Erstelle `packages/ai-adapter/src/optimization/token-counter.ts`:

```typescript
import { encoding_for_model, Tiktoken } from 'tiktoken';

export class TokenCounter {
  private encoders: Map<string, Tiktoken> = new Map();
  private modelLimits: Map<string, number> = new Map([
    ['gpt-4', 128000],
    ['gpt-4-turbo', 128000],
    ['gpt-3.5-turbo', 16385],
    ['claude-3-opus', 200000],
    ['claude-3-sonnet', 200000],
    ['gemini-pro', 32768],
    ['llama-2-70b', 4096]
  ]);

  /**
   * Count tokens for a specific model
   */
  countTokens(text: string, model: string): number {
    const encoder = this.getEncoder(model);
    
    try {
      const tokens = encoder.encode(text);
      return tokens.length;
    } catch (error) {
      // Fallback to approximation
      return this.approximateTokens(text);
    }
  }

  /**
   * Count tokens for messages array
   */
  countMessages(messages: AIMessage[], model: string): number {
    let totalTokens = 0;
    
    // Each message has overhead tokens
    const messageOverhead = this.getMessageOverhead(model);
    
    for (const message of messages) {
      // Role tokens
      totalTokens += this.countTokens(message.role, model);
      
      // Content tokens
      if (typeof message.content === 'string') {
        totalTokens += this.countTokens(message.content, model);
      } else {
        // Handle array content (vision models)
        totalTokens += this.countComplexContent(message.content, model);
      }
      
      // Message overhead
      totalTokens += messageOverhead;
    }
    
    // Add base prompt tokens
    totalTokens += 3; // Every request has ~3 token overhead
    
    return totalTokens;
  }

  /**
   * Get available token budget
   */
  getTokenBudget(model: string, usedTokens: number): number {
    const limit = this.modelLimits.get(model) || 4096;
    const safetyMargin = Math.min(1000, limit * 0.1); // 10% or 1000 tokens
    return limit - usedTokens - safetyMargin;
  }

  /**
   * Analyze token distribution
   */
  analyzeTokenDistribution(messages: AIMessage[], model: string): TokenAnalysis {
    const analysis: TokenAnalysis = {
      total: 0,
      byRole: {},
      byMessage: [],
      systemPrompt: 0,
      userContext: 0,
      assistantContext: 0,
      avgMessageLength: 0
    };

    for (const message of messages) {
      const tokens = this.countTokens(
        typeof message.content === 'string' ? message.content : JSON.stringify(message.content),
        model
      );
      
      analysis.total += tokens;
      analysis.byRole[message.role] = (analysis.byRole[message.role] || 0) + tokens;
      analysis.byMessage.push({ role: message.role, tokens });
      
      if (message.role === 'system') {
        analysis.systemPrompt += tokens;
      } else if (message.role === 'user') {
        analysis.userContext += tokens;
      } else if (message.role === 'assistant') {
        analysis.assistantContext += tokens;
      }
    }

    analysis.avgMessageLength = analysis.total / messages.length;
    
    return analysis;
  }

  private getEncoder(model: string): Tiktoken {
    if (!this.encoders.has(model)) {
      try {
        const encoder = encoding_for_model(model as any);
        this.encoders.set(model, encoder);
      } catch {
        // Use cl100k_base as default
        const encoder = encoding_for_model('gpt-3.5-turbo');
        this.encoders.set(model, encoder);
      }
    }
    
    return this.encoders.get(model)!;
  }

  private approximateTokens(text: string): number {
    // Rough approximation: 1 token ≈ 4 characters
    return Math.ceil(text.length / 4);
  }

  private getMessageOverhead(model: string): number {
    // Different models have different message formatting overhead
    if (model.startsWith('gpt')) return 4;
    if (model.startsWith('claude')) return 3;
    return 2;
  }

  private countComplexContent(content: any[], model: string): number {
    let tokens = 0;
    
    for (const item of content) {
      if (item.type === 'text') {
        tokens += this.countTokens(item.text, model);
      } else if (item.type === 'image') {
        // Images have fixed token cost
        tokens += 85; // Base64 encoded image ≈ 85 tokens
      }
    }
    
    return tokens;
  }

  /**
   * Clean up encoders
   */
  dispose(): void {
    this.encoders.forEach(encoder => encoder.free());
    this.encoders.clear();
  }
}

interface TokenAnalysis {
  total: number;
  byRole: Record<string, number>;
  byMessage: Array<{ role: string; tokens: number }>;
  systemPrompt: number;
  userContext: number;
  assistantContext: number;
  avgMessageLength: number;
}
```

### Schritt 2: Prompt Compression
Erstelle `packages/ai-adapter/src/optimization/prompt-compressor.ts`:

```typescript
export class PromptCompressor {
  /**
   * Compress text while preserving meaning
   */
  compress(text: string): string {
    let compressed = text;
    
    // Remove excessive whitespace
    compressed = this.normalizeWhitespace(compressed);
    
    // Remove redundant words
    compressed = this.removeRedundancy(compressed);
    
    // Compress common patterns
    compressed = this.compressPatterns(compressed);
    
    // Abbreviate where safe
    compressed = this.abbreviate(compressed);
    
    return compressed;
  }

  /**
   * Normalize whitespace
   */
  private normalizeWhitespace(text: string): string {
    return text
      .replace(/\s+/g, ' ') // Multiple spaces to single
      .replace(/\n{3,}/g, '\n\n') // Multiple newlines to double
      .replace(/\t/g, ' ') // Tabs to spaces
      .trim();
  }

  /**
   * Remove redundant words and phrases
   */
  private removeRedundancy(text: string): string {
    const redundantPhrases = [
      /\b(basically|essentially|actually|really)\b/gi,
      /\b(in order to)\b/gi, // -> "to"
      /\b(at this point in time)\b/gi, // -> "now"
      /\b(due to the fact that)\b/gi, // -> "because"
      /\b(in the event that)\b/gi, // -> "if"
    ];

    let result = text;
    
    redundantPhrases.forEach(pattern => {
      result = result.replace(pattern, '');
    });
    
    // Fix double spaces created by removal
    result = result.replace(/\s+/g, ' ');
    
    return result;
  }

  /**
   * Compress common patterns
   */
  private compressPatterns(text: string): string {
    const patterns: [RegExp, string][] = [
      [/\bdo not\b/gi, "don't"],
      [/\bcannot\b/gi, "can't"],
      [/\bwill not\b/gi, "won't"],
      [/\byou are\b/gi, "you're"],
      [/\bit is\b/gi, "it's"],
      [/\bthat is\b/gi, "that's"],
      // Technical compressions
      [/\bapplication programming interface\b/gi, "API"],
      [/\buser interface\b/gi, "UI"],
      [/\buser experience\b/gi, "UX"],
      [/\bartificial intelligence\b/gi, "AI"],
      [/\bmachine learning\b/gi, "ML"],
    ];

    let result = text;
    
    patterns.forEach(([pattern, replacement]) => {
      result = result.replace(pattern, replacement);
    });
    
    return result;
  }

  /**
   * Safe abbreviation of common terms
   */
  private abbreviate(text: string): string {
    const abbreviations: Map<string, string> = new Map([
      ['example', 'e.g.'],
      ['for example', 'e.g.'],
      ['that is to say', 'i.e.'],
      ['et cetera', 'etc.'],
      ['versus', 'vs.'],
      ['approximately', '~'],
      ['greater than', '>'],
      ['less than', '<'],
    ]);

    let result = text;
    
    abbreviations.forEach((abbr, full) => {
      const pattern = new RegExp(`\\b${full}\\b`, 'gi');
      result = result.replace(pattern, abbr);
    });
    
    return result;
  }

  /**
   * Extract key points from verbose text
   */
  extractKeyPoints(text: string, maxTokens: number): string {
    const sentences = text.split(/[.!?]+/);
    const scoredSentences = sentences.map(sentence => ({
      text: sentence.trim(),
      score: this.calculateImportance(sentence)
    }));

    // Sort by importance
    scoredSentences.sort((a, b) => b.score - a.score);

    // Take most important sentences within token budget
    const result: string[] = [];
    let currentTokens = 0;
    
    for (const { text } of scoredSentences) {
      const sentenceTokens = Math.ceil(text.length / 4);
      if (currentTokens + sentenceTokens <= maxTokens) {
        result.push(text);
        currentTokens += sentenceTokens;
      } else {
        break;
      }
    }

    return result.join('. ') + '.';
  }

  private calculateImportance(sentence: string): number {
    let score = 0;
    
    // Keywords increase importance
    const importantKeywords = [
      'important', 'critical', 'essential', 'must', 'required',
      'error', 'warning', 'problem', 'issue', 'solution'
    ];
    
    importantKeywords.forEach(keyword => {
      if (sentence.toLowerCase().includes(keyword)) {
        score += 10;
      }
    });
    
    // Questions are important
    if (sentence.includes('?')) score += 5;
    
    // First and last sentences are often important
    if (sentence.length > 50) score += 3;
    
    // Sentences with numbers/data are important
    if (/\d+/.test(sentence)) score += 3;
    
    return score;
  }
}
```

### Schritt 3: Context Window Manager
Erstelle `packages/ai-adapter/src/optimization/context-manager.ts`:

```typescript
import { TokenCounter } from './token-counter';
import { PromptCompressor } from './prompt-compressor';

export class ContextWindowManager {
  private tokenCounter: TokenCounter;
  private compressor: PromptCompressor;

  constructor() {
    this.tokenCounter = new TokenCounter();
    this.compressor = new PromptCompressor();
  }

  /**
   * Optimize messages to fit within token budget
   */
  optimizeMessages(
    messages: AIMessage[],
    model: string,
    targetTokens?: number
  ): OptimizedMessages {
    const modelLimit = this.tokenCounter.getTokenBudget(model, 0);
    const target = targetTokens || modelLimit * 0.8; // Use 80% of limit

    // Analyze current token usage
    const analysis = this.tokenCounter.analyzeTokenDistribution(messages, model);
    
    if (analysis.total <= target) {
      // Already within budget
      return {
        messages,
        originalTokens: analysis.total,
        optimizedTokens: analysis.total,
        reduction: 0
      };
    }

    // Apply optimization strategies
    let optimized = [...messages];
    
    // Strategy 1: Compress all messages
    optimized = this.compressMessages(optimized);
    
    // Strategy 2: Truncate middle context
    optimized = this.truncateMiddleContext(optimized, model, target);
    
    // Strategy 3: Summarize old messages
    optimized = this.summarizeOldMessages(optimized, model, target);
    
    // Strategy 4: Remove redundant information
    optimized = this.removeRedundancy(optimized);

    const newTokens = this.tokenCounter.countMessages(optimized, model);
    
    return {
      messages: optimized,
      originalTokens: analysis.total,
      optimizedTokens: newTokens,
      reduction: ((analysis.total - newTokens) / analysis.total) * 100
    };
  }

  /**
   * Compress all messages
   */
  private compressMessages(messages: AIMessage[]): AIMessage[] {
    return messages.map(msg => ({
      ...msg,
      content: typeof msg.content === 'string' 
        ? this.compressor.compress(msg.content)
        : msg.content
    }));
  }

  /**
   * Truncate middle context while preserving recent messages
   */
  private truncateMiddleContext(
    messages: AIMessage[],
    model: string,
    targetTokens: number
  ): AIMessage[] {
    if (messages.length <= 3) return messages;

    // Always keep system message
    const systemMsg = messages.find(m => m.role === 'system');
    const nonSystemMessages = messages.filter(m => m.role !== 'system');

    // Keep first and last few messages
    const keepStart = 2;
    const keepEnd = Math.min(4, Math.floor(nonSystemMessages.length / 2));
    
    const result: AIMessage[] = [];
    
    if (systemMsg) result.push(systemMsg);
    
    // Add first messages
    result.push(...nonSystemMessages.slice(0, keepStart));
    
    // Add truncation notice
    if (nonSystemMessages.length > keepStart + keepEnd) {
      result.push({
        role: 'system',
        content: `[${nonSystemMessages.length - keepStart - keepEnd} messages omitted for brevity]`
      });
    }
    
    // Add last messages
    result.push(...nonSystemMessages.slice(-keepEnd));
    
    return result;
  }

  /**
   * Summarize old conversation chunks
   */
  private summarizeOldMessages(
    messages: AIMessage[],
    model: string,
    targetTokens: number
  ): AIMessage[] {
    if (messages.length <= 5) return messages;

    const result: AIMessage[] = [];
    const systemMsg = messages.find(m => m.role === 'system');
    
    if (systemMsg) result.push(systemMsg);

    // Group old messages into chunks
    const chunkSize = 4;
    const oldMessages = messages.filter(m => m.role !== 'system').slice(0, -4);
    const recentMessages = messages.filter(m => m.role !== 'system').slice(-4);

    // Summarize old messages in chunks
    for (let i = 0; i < oldMessages.length; i += chunkSize) {
      const chunk = oldMessages.slice(i, i + chunkSize);
      const summary = this.summarizeChunk(chunk);
      
      if (summary) {
        result.push({
          role: 'system',
          content: `[Summary of messages ${i + 1}-${i + chunk.length}]: ${summary}`
        });
      }
    }

    // Keep recent messages intact
    result.push(...recentMessages);
    
    return result;
  }

  /**
   * Create summary of message chunk
   */
  private summarizeChunk(messages: AIMessage[]): string {
    const points: string[] = [];
    
    for (const msg of messages) {
      const content = typeof msg.content === 'string' ? msg.content : '';
      
      if (msg.role === 'user' && content.includes('?')) {
        // Questions are important
        points.push(`User asked: ${this.compressor.extractKeyPoints(content, 50)}`);
      } else if (msg.role === 'assistant') {
        // Key points from responses
        const keyPoints = this.compressor.extractKeyPoints(content, 100);
        if (keyPoints.length > 20) {
          points.push(`Assistant: ${keyPoints}`);
        }
      }
    }

    return points.join('; ');
  }

  /**
   * Remove redundant information across messages
   */
  private removeRedundancy(messages: AIMessage[]): AIMessage[] {
    const seen = new Set<string>();
    
    return messages.map(msg => {
      if (typeof msg.content !== 'string') return msg;
      
      // Split into sentences
      const sentences = msg.content.split(/[.!?]+/);
      const unique: string[] = [];
      
      for (const sentence of sentences) {
        const normalized = sentence.trim().toLowerCase();
        
        // Skip if we've seen very similar content
        if (!this.isSimilarToSeen(normalized, seen)) {
          unique.push(sentence);
          seen.add(normalized);
        }
      }
      
      return {
        ...msg,
        content: unique.join('. ')
      };
    });
  }

  private isSimilarToSeen(text: string, seen: Set<string>): boolean {
    // Simple similarity check - in production use better algorithm
    for (const existing of seen) {
      if (this.calculateSimilarity(text, existing) > 0.8) {
        return true;
      }
    }
    return false;
  }

  private calculateSimilarity(a: string, b: string): number {
    // Simplified Jaccard similarity
    const wordsA = new Set(a.split(' '));
    const wordsB = new Set(b.split(' '));
    
    const intersection = new Set([...wordsA].filter(x => wordsB.has(x)));
    const union = new Set([...wordsA, ...wordsB]);
    
    return intersection.size / union.size;
  }

  /**
   * Clean up resources
   */
  dispose(): void {
    this.tokenCounter.dispose();
  }
}

interface OptimizedMessages {
  messages: AIMessage[];
  originalTokens: number;
  optimizedTokens: number;
  reduction: number; // Percentage
}
```

### Schritt 4: Integration in AI Adapter
Update `packages/ai-adapter/src/ai-adapter.ts`:

```typescript
import { ContextWindowManager } from './optimization/context-manager';
import { TokenCounter } from './optimization/token-counter';

export class AIAdapter {
  private contextManager: ContextWindowManager;
  private tokenCounter: TokenCounter;
  private tokenBudgets: Map<string, number> = new Map();

  constructor(configs: AdapterConfig[]) {
    this.contextManager = new ContextWindowManager();
    this.tokenCounter = new TokenCounter();
    
    // Set token budgets per provider
    this.tokenBudgets.set('default', 4000);
    this.tokenBudgets.set('extended', 8000);
    this.tokenBudgets.set('max', 16000);
  }

  async complete(options: AICompletionOptions): Promise<AICompletion> {
    // Optimize messages before sending
    const optimized = this.contextManager.optimizeMessages(
      options.messages,
      options.model,
      options.maxTokens
    );

    console.log(
      `Token optimization: ${optimized.originalTokens} → ${optimized.optimizedTokens} ` +
      `(${optimized.reduction.toFixed(1)}% reduction)`
    );

    // Use optimized messages
    const response = await this.provider.complete({
      ...options,
      messages: optimized.messages
    });

    // Track token usage
    this.trackTokenUsage(options.model, optimized.optimizedTokens, response.usage?.total_tokens);

    return response;
  }

  private trackTokenUsage(model: string, input: number, total?: number): void {
    const cost = this.calculateCost(model, input, total);
    
    console.log(`Token usage for ${model}:`, {
      input,
      total,
      estimatedCost: `$${cost.toFixed(4)}`
    });
  }

  private calculateCost(model: string, inputTokens: number, totalTokens?: number): number {
    // Rough cost estimates per 1K tokens
    const costs: Record<string, { input: number; output: number }> = {
      'gpt-4': { input: 0.03, output: 0.06 },
      'gpt-3.5-turbo': { input: 0.001, output: 0.002 },
      'claude-3-opus': { input: 0.015, output: 0.075 },
      'claude-3-sonnet': { input: 0.003, output: 0.015 }
    };

    const modelCost = costs[model] || { input: 0.001, output: 0.002 };
    const inputCost = (inputTokens / 1000) * modelCost.input;
    
    if (totalTokens) {
      const outputTokens = totalTokens - inputTokens;
      const outputCost = (outputTokens / 1000) * modelCost.output;
      return inputCost + outputCost;
    }
    
    return inputCost;
  }
}
```

### Schritt 5: Tests
Erstelle `packages/ai-adapter/src/optimization/__tests__/token-optimization.test.ts`:

```typescript
describe('Token Optimization', () => {
  let contextManager: ContextWindowManager;
  let compressor: PromptCompressor;

  beforeEach(() => {
    contextManager = new ContextWindowManager();
    compressor = new PromptCompressor();
  });

  test('should compress text effectively', () => {
    const original = 'In order to understand the application programming interface...';
    const compressed = compressor.compress(original);
    
    expect(compressed).toBe('To understand the API...');
    expect(compressed.length).toBeLessThan(original.length);
  });

  test('should optimize messages within budget', () => {
    const messages: AIMessage[] = [
      { role: 'system', content: 'You are a helpful assistant.' },
      ...Array(20).fill(0).map((_, i) => ({
        role: i % 2 === 0 ? 'user' : 'assistant',
        content: `This is message ${i} with some content that takes up tokens.`
      }))
    ];

    const result = contextManager.optimizeMessages(messages, 'gpt-3.5-turbo', 1000);
    
    expect(result.optimizedTokens).toBeLessThan(result.originalTokens);
    expect(result.reduction).toBeGreaterThan(20);
  });

  test('should preserve important context', () => {
    const messages: AIMessage[] = [
      { role: 'system', content: 'System prompt' },
      { role: 'user', content: 'First question?' },
      { role: 'assistant', content: 'Answer' },
      { role: 'user', content: 'Current important question?' }
    ];

    const result = contextManager.optimizeMessages(messages, 'gpt-4', 100);
    
    // Should keep system and recent messages
    expect(result.messages.some(m => m.content.includes('System prompt'))).toBe(true);
    expect(result.messages.some(m => m.content.includes('Current important'))).toBe(true);
  });
});
```

## Verifizierung

### Test 1: Token Reduction Rate
```bash
npm test -- token-reduction
# Should show 30-50% reduction on average
```

### Test 2: Cost Savings
```typescript
const before = calculateCost(originalTokens);
const after = calculateCost(optimizedTokens);
console.log(`Cost savings: $${(before - after).toFixed(2)} per request`);
```

## Erfolgskriterien
- [ ] Token counting accurate for all models
- [ ] Prompt compression reduces tokens by 30%+
- [ ] Context window management prevents overflows
- [ ] Important information preserved
- [ ] Cost tracking implemented
- [ ] No quality degradation in responses

## Zeitschätzung
- Token Counter: 5 Minuten
- Prompt Compressor: 6 Minuten
- Context Manager: 6 Minuten
- Integration: 3 Minuten
- Tests: 3 Minuten
- **Total: 23 Minuten**

## Nächster Schritt
Nach erfolgreicher Token Optimization → [12.11.4 Streaming Optimization](./12.11.4-streaming-optimization.md)