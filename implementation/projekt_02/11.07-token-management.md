# 11.07 Token Management

## Ziel
Implementierung eines umfassenden Token-Management-Systems für Kosten-Tracking, Usage-Limits und Token-Counting.

## Umfang
- Token counting für verschiedene Modelle
- Cost calculation und tracking
- Usage limits und quotas
- Token optimization strategies
- Analytics und reporting

## Tasks

### 1. Token Counter Implementations
```typescript
// packages/ai-adapter/src/tokens/counter.ts
export interface TokenCounter {
  count(text: string): number;
  countMessages(messages: Message[]): number;
  estimateTokens(request: CompletionRequest): number;
}

// GPT Token Counter (tiktoken)
export class GPTTokenCounter implements TokenCounter {
  private encoder: any; // tiktoken encoder
  
  constructor(model: string = 'gpt-4') {
    // Dynamic import of tiktoken
    this.initEncoder(model);
  }
  
  private async initEncoder(model: string) {
    const { encoding_for_model } = await import('tiktoken');
    this.encoder = encoding_for_model(model);
  }
  
  count(text: string): number {
    if (!this.encoder) return this.estimateFallback(text);
    return this.encoder.encode(text).length;
  }
  
  countMessages(messages: Message[]): number {
    let tokens = 0;
    
    for (const message of messages) {
      // Each message has overhead tokens
      tokens += 4; // <im_start>{role}\n
      tokens += this.count(message.role);
      tokens += this.count(message.content);
      tokens += 2; // <im_end>\n
    }
    
    tokens += 2; // <im_start>assistant
    return tokens;
  }
  
  estimateTokens(request: CompletionRequest): number {
    const messageTokens = this.countMessages(request.messages);
    const maxTokens = request.maxTokens || 1000;
    return messageTokens + maxTokens;
  }
  
  private estimateFallback(text: string): number {
    // Rough estimate: ~4 characters per token
    return Math.ceil(text.length / 4);
  }
}

// Claude Token Counter
export class ClaudeTokenCounter implements TokenCounter {
  count(text: string): number {
    // Claude uses a similar tokenization to GPT
    // More accurate would be to use Anthropic's tokenizer
    return Math.ceil(text.length / 4);
  }
  
  countMessages(messages: Message[]): number {
    let tokens = 0;
    
    for (const message of messages) {
      tokens += this.count(message.content);
      
      // Add overhead for message structure
      if (message.role === 'system') {
        tokens += 10; // System message overhead
      } else {
        tokens += 5; // Regular message overhead
      }
    }
    
    return tokens;
  }
  
  estimateTokens(request: CompletionRequest): number {
    const messageTokens = this.countMessages(request.messages);
    const maxTokens = request.maxTokens || 4096;
    return messageTokens + maxTokens;
  }
}
```

### 2. Cost Calculator
```typescript
// packages/ai-adapter/src/tokens/pricing.ts
export interface ModelPricing {
  inputPricePerToken: number;
  outputPricePerToken: number;
  currency: 'USD' | 'EUR';
}

export const MODEL_PRICING: Record<string, ModelPricing> = {
  // OpenAI
  'gpt-4-turbo': {
    inputPricePerToken: 0.00001, // $10 per 1M tokens
    outputPricePerToken: 0.00003, // $30 per 1M tokens
    currency: 'USD',
  },
  'gpt-3.5-turbo': {
    inputPricePerToken: 0.0000005, // $0.50 per 1M tokens
    outputPricePerToken: 0.0000015, // $1.50 per 1M tokens
    currency: 'USD',
  },
  
  // Anthropic
  'claude-3-opus': {
    inputPricePerToken: 0.000015, // $15 per 1M tokens
    outputPricePerToken: 0.000075, // $75 per 1M tokens
    currency: 'USD',
  },
  'claude-3-sonnet': {
    inputPricePerToken: 0.000003, // $3 per 1M tokens
    outputPricePerToken: 0.000015, // $15 per 1M tokens
    currency: 'USD',
  },
  
  // Google
  'gemini-pro': {
    inputPricePerToken: 0.00000025, // $0.25 per 1M tokens
    outputPricePerToken: 0.00000125, // $1.25 per 1M tokens
    currency: 'USD',
  },
};

export class CostCalculator {
  calculateCost(usage: TokenUsage, model: string): Cost {
    const pricing = MODEL_PRICING[model];
    if (!pricing) {
      throw new Error(`No pricing information for model: ${model}`);
    }
    
    const inputCost = usage.promptTokens * pricing.inputPricePerToken;
    const outputCost = usage.completionTokens * pricing.outputPricePerToken;
    const totalCost = inputCost + outputCost;
    
    return {
      inputCost,
      outputCost,
      totalCost,
      currency: pricing.currency,
      breakdown: {
        inputTokens: usage.promptTokens,
        outputTokens: usage.completionTokens,
        totalTokens: usage.totalTokens,
        pricePerInputToken: pricing.inputPricePerToken,
        pricePerOutputToken: pricing.outputPricePerToken,
      },
    };
  }
  
  estimateCost(request: CompletionRequest, model: string): Cost {
    const counter = this.getCounter(model);
    const estimatedInput = counter.countMessages(request.messages);
    const estimatedOutput = request.maxTokens || 1000;
    
    return this.calculateCost(
      {
        promptTokens: estimatedInput,
        completionTokens: estimatedOutput,
        totalTokens: estimatedInput + estimatedOutput,
      },
      model
    );
  }
  
  private getCounter(model: string): TokenCounter {
    if (model.startsWith('gpt')) {
      return new GPTTokenCounter(model);
    } else if (model.startsWith('claude')) {
      return new ClaudeTokenCounter();
    }
    // Default fallback
    return new GPTTokenCounter();
  }
}
```

### 3. Usage Tracking & Limits
```typescript
// packages/ai-adapter/src/tokens/usage-tracker.ts
export interface UsageQuota {
  maxTokensPerMinute?: number;
  maxTokensPerHour?: number;
  maxTokensPerDay?: number;
  maxTokensPerMonth?: number;
  maxCostPerDay?: number;
  maxCostPerMonth?: number;
}

export class UsageTracker {
  private usage = new Map<string, TokenUsage[]>();
  private costs = new Map<string, Cost[]>();
  
  constructor(
    private quotas: Record<string, UsageQuota> = {},
    private storage?: UsageStorage
  ) {
    // Load historical usage from storage if available
    this.loadHistoricalUsage();
  }
  
  async trackUsage(
    userId: string,
    usage: TokenUsage,
    model: string
  ): Promise<void> {
    const timestamp = Date.now();
    
    // Track token usage
    if (!this.usage.has(userId)) {
      this.usage.set(userId, []);
    }
    this.usage.get(userId)!.push({ ...usage, timestamp });
    
    // Calculate and track cost
    const calculator = new CostCalculator();
    const cost = calculator.calculateCost(usage, model);
    
    if (!this.costs.has(userId)) {
      this.costs.set(userId, []);
    }
    this.costs.get(userId)!.push({ ...cost, timestamp });
    
    // Persist to storage
    await this.storage?.save(userId, { usage, cost, timestamp });
    
    // Clean up old data (keep last 30 days)
    this.cleanupOldData(userId);
  }
  
  checkQuota(userId: string, estimatedUsage: TokenUsage): QuotaCheck {
    const quota = this.quotas[userId] || this.quotas.default;
    if (!quota) return { allowed: true };
    
    const now = Date.now();
    const userUsage = this.usage.get(userId) || [];
    
    // Check token limits
    const checks = [
      this.checkTimeWindow(userUsage, now, 60000, quota.maxTokensPerMinute),
      this.checkTimeWindow(userUsage, now, 3600000, quota.maxTokensPerHour),
      this.checkTimeWindow(userUsage, now, 86400000, quota.maxTokensPerDay),
      this.checkTimeWindow(userUsage, now, 2592000000, quota.maxTokensPerMonth),
    ];
    
    for (const check of checks) {
      if (!check.allowed) {
        return check;
      }
    }
    
    // Check cost limits
    const userCosts = this.costs.get(userId) || [];
    const costChecks = [
      this.checkCostWindow(userCosts, now, 86400000, quota.maxCostPerDay),
      this.checkCostWindow(userCosts, now, 2592000000, quota.maxCostPerMonth),
    ];
    
    for (const check of costChecks) {
      if (!check.allowed) {
        return check;
      }
    }
    
    return { allowed: true };
  }
  
  private checkTimeWindow(
    usage: (TokenUsage & { timestamp: number })[],
    now: number,
    windowMs: number,
    limit?: number
  ): QuotaCheck {
    if (!limit) return { allowed: true };
    
    const windowUsage = usage
      .filter(u => now - u.timestamp < windowMs)
      .reduce((sum, u) => sum + u.totalTokens, 0);
    
    if (windowUsage >= limit) {
      return {
        allowed: false,
        reason: `Token limit exceeded: ${windowUsage}/${limit} in ${windowMs / 60000} minutes`,
        resetAt: now + windowMs,
      };
    }
    
    return { allowed: true };
  }
  
  private checkCostWindow(
    costs: (Cost & { timestamp: number })[],
    now: number,
    windowMs: number,
    limit?: number
  ): QuotaCheck {
    if (!limit) return { allowed: true };
    
    const windowCost = costs
      .filter(c => now - c.timestamp < windowMs)
      .reduce((sum, c) => sum + c.totalCost, 0);
    
    if (windowCost >= limit) {
      return {
        allowed: false,
        reason: `Cost limit exceeded: $${windowCost.toFixed(2)}/$${limit} in ${windowMs / 86400000} days`,
        resetAt: now + windowMs,
      };
    }
    
    return { allowed: true };
  }
  
  getUsageStats(userId: string, timeWindow?: number): UsageStats {
    const now = Date.now();
    const userUsage = this.usage.get(userId) || [];
    const userCosts = this.costs.get(userId) || [];
    
    const relevantUsage = timeWindow
      ? userUsage.filter(u => now - u.timestamp < timeWindow)
      : userUsage;
    
    const relevantCosts = timeWindow
      ? userCosts.filter(c => now - c.timestamp < timeWindow)
      : userCosts;
    
    return {
      totalTokens: relevantUsage.reduce((sum, u) => sum + u.totalTokens, 0),
      totalCost: relevantCosts.reduce((sum, c) => sum + c.totalCost, 0),
      requestCount: relevantUsage.length,
      averageTokensPerRequest: relevantUsage.length > 0
        ? relevantUsage.reduce((sum, u) => sum + u.totalTokens, 0) / relevantUsage.length
        : 0,
      timeWindow,
    };
  }
}
```

### 4. Token Optimization
```typescript
// packages/ai-adapter/src/tokens/optimizer.ts
export class TokenOptimizer {
  // Compress messages by removing redundancy
  compressMessages(messages: Message[], targetReduction = 0.2): Message[] {
    return messages.map(msg => ({
      ...msg,
      content: this.compressText(msg.content, targetReduction),
    }));
  }
  
  private compressText(text: string, targetReduction: number): string {
    // Remove excessive whitespace
    let compressed = text.replace(/\s+/g, ' ').trim();
    
    // Remove redundant punctuation
    compressed = compressed.replace(/([.!?])\1+/g, '$1');
    
    // Summarize if still too long
    if (compressed.length > 1000) {
      compressed = this.summarize(compressed, targetReduction);
    }
    
    return compressed;
  }
  
  // Truncate conversation history to fit within token limits
  truncateHistory(
    messages: Message[],
    maxTokens: number,
    counter: TokenCounter,
    strategy: 'fifo' | 'lifo' | 'smart' = 'smart'
  ): Message[] {
    const currentTokens = counter.countMessages(messages);
    
    if (currentTokens <= maxTokens) {
      return messages;
    }
    
    switch (strategy) {
      case 'fifo':
        return this.truncateFIFO(messages, maxTokens, counter);
        
      case 'lifo':
        return this.truncateLIFO(messages, maxTokens, counter);
        
      case 'smart':
      default:
        return this.truncateSmart(messages, maxTokens, counter);
    }
  }
  
  private truncateSmart(
    messages: Message[],
    maxTokens: number,
    counter: TokenCounter
  ): Message[] {
    // Keep system messages and recent messages
    const system = messages.filter(m => m.role === 'system');
    const nonSystem = messages.filter(m => m.role !== 'system');
    
    // Keep last N messages that fit
    const result: Message[] = [...system];
    let tokens = counter.countMessages(result);
    
    for (let i = nonSystem.length - 1; i >= 0; i--) {
      const messageTokens = counter.count(nonSystem[i].content);
      if (tokens + messageTokens <= maxTokens) {
        result.push(nonSystem[i]);
        tokens += messageTokens;
      } else {
        break;
      }
    }
    
    // Add summary of truncated messages if significant
    if (nonSystem.length - result.length > 5) {
      result.unshift({
        role: 'system',
        content: `[Previous ${nonSystem.length - result.length} messages summarized]`,
      });
    }
    
    return result;
  }
  
  private summarize(text: string, targetReduction: number): string {
    // Simple summarization (in production, use AI)
    const sentences = text.match(/[^.!?]+[.!?]+/g) || [];
    const targetLength = Math.floor(sentences.length * (1 - targetReduction));
    
    // Keep most important sentences (first, last, and evenly distributed)
    const kept: string[] = [];
    if (sentences.length > 0) {
      kept.push(sentences[0]); // First sentence
      
      const step = Math.floor(sentences.length / targetLength);
      for (let i = step; i < sentences.length - 1; i += step) {
        kept.push(sentences[i]);
      }
      
      if (sentences.length > 1) {
        kept.push(sentences[sentences.length - 1]); // Last sentence
      }
    }
    
    return kept.join(' ');
  }
}
```

### 5. Analytics & Reporting
```typescript
// packages/ai-adapter/src/tokens/analytics.ts
export class UsageAnalytics {
  generateReport(
    tracker: UsageTracker,
    userIds: string[],
    timeRange: { start: number; end: number }
  ): UsageReport {
    const report: UsageReport = {
      timeRange,
      totalUsers: userIds.length,
      totalTokens: 0,
      totalCost: 0,
      byUser: {},
      byModel: {},
      byTimeOfDay: new Array(24).fill(0),
      topUsers: [],
    };
    
    for (const userId of userIds) {
      const stats = tracker.getUsageStats(
        userId,
        timeRange.end - timeRange.start
      );
      
      report.totalTokens += stats.totalTokens;
      report.totalCost += stats.totalCost;
      
      report.byUser[userId] = stats;
    }
    
    // Calculate top users
    report.topUsers = Object.entries(report.byUser)
      .sort((a, b) => b[1].totalTokens - a[1].totalTokens)
      .slice(0, 10)
      .map(([userId, stats]) => ({ userId, ...stats }));
    
    return report;
  }
  
  predictCost(
    historicalUsage: TokenUsage[],
    projectionDays: number
  ): CostPrediction {
    // Calculate daily average
    const dailyTokens = this.calculateDailyAverage(historicalUsage);
    
    // Apply growth factor (optional)
    const growthFactor = 1.1; // 10% growth
    const projectedDailyTokens = dailyTokens * growthFactor;
    
    // Calculate cost for different models
    const predictions: Record<string, number> = {};
    const calculator = new CostCalculator();
    
    for (const [model, pricing] of Object.entries(MODEL_PRICING)) {
      const dailyCost = projectedDailyTokens * 
        ((pricing.inputPricePerToken + pricing.outputPricePerToken) / 2);
      predictions[model] = dailyCost * projectionDays;
    }
    
    return {
      projectionDays,
      projectedTokens: projectedDailyTokens * projectionDays,
      costByModel: predictions,
      assumptions: {
        growthFactor,
        averageTokensPerDay: dailyTokens,
      },
    };
  }
}
```

## Dateien zu erstellen/ändern
- `packages/ai-adapter/src/tokens/counter.ts` - Token counting
- `packages/ai-adapter/src/tokens/pricing.ts` - Cost calculation
- `packages/ai-adapter/src/tokens/usage-tracker.ts` - Usage tracking
- `packages/ai-adapter/src/tokens/optimizer.ts` - Token optimization
- `packages/ai-adapter/src/tokens/analytics.ts` - Analytics

## Akzeptanzkriterien
- [ ] Token counting für alle Provider
- [ ] Cost calculation mit aktuellen Preisen
- [ ] Usage tracking und quota enforcement
- [ ] Token optimization strategies
- [ ] Analytics und reporting
- [ ] Persistent storage support
- [ ] Real-time quota checking
- [ ] Tests für alle Komponenten

## Abhängigkeiten
- tiktoken npm package für GPT token counting
- Tasks 11.01-11.04 (Provider implementations)
- Storage solution (optional)

## Geschätzter Aufwand
75-90 Minuten