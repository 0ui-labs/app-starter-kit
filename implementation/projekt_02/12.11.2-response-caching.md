# Sub-Task 12.11.2: Response Caching für AI Adapter

## Ziel
Einrichtung eines intelligenten In-Memory- und optionalen Redis-Caches für identische, wiederholte Anfragen, um Kosten und Latenz zu sparen. Reduziert API-Kosten um 40-60% bei häufigen Queries.

## Voraussetzungen
- AI Adapter Grundstruktur implementiert
- Connection Pooling (12.11.1) abgeschlossen
- Optional: Redis für verteiltes Caching

## Umfang
- Multi-Layer Cache Architektur (Memory + Redis)
- Intelligente Cache-Key-Generierung
- TTL-Management und Cache-Invalidierung
- Cache-Hit-Rate Monitoring

## Implementierung

### Schritt 1: Cache Interface und Types
Erstelle `packages/ai-adapter/src/cache/types.ts`:

```typescript
export interface CacheEntry<T = any> {
  data: T;
  timestamp: number;
  ttl: number;
  hits: number;
  size: number;
}

export interface CacheOptions {
  ttl?: number; // Time to live in milliseconds
  maxSize?: number; // Max cache size in MB
  maxEntries?: number; // Max number of entries
  enableRedis?: boolean;
  redisUrl?: string;
}

export interface CacheStats {
  hits: number;
  misses: number;
  entries: number;
  sizeBytes: number;
  hitRate: number;
  avgResponseTime: number;
  costSavings: number; // Estimated $ saved
}

export interface ICacheProvider {
  get<T>(key: string): Promise<T | null>;
  set<T>(key: string, value: T, ttl?: number): Promise<void>;
  delete(key: string): Promise<void>;
  clear(): Promise<void>;
  getStats(): CacheStats;
}
```

### Schritt 2: LRU Memory Cache Implementation
Erstelle `packages/ai-adapter/src/cache/memory-cache.ts`:

```typescript
import { ICacheProvider, CacheEntry, CacheOptions, CacheStats } from './types';
import { LRUCache } from 'lru-cache';

export class MemoryCache implements ICacheProvider {
  private cache: LRUCache<string, CacheEntry>;
  private stats: CacheStats = {
    hits: 0,
    misses: 0,
    entries: 0,
    sizeBytes: 0,
    hitRate: 0,
    avgResponseTime: 0,
    costSavings: 0
  };

  constructor(private options: CacheOptions = {}) {
    this.cache = new LRUCache<string, CacheEntry>({
      max: options.maxEntries || 1000,
      maxSize: (options.maxSize || 100) * 1024 * 1024, // Convert MB to bytes
      sizeCalculation: (entry) => entry.size,
      ttl: options.ttl || 3600000, // Default 1 hour
      updateAgeOnGet: true,
      updateAgeOnHas: true,
      
      // Disposal function for cleanup
      dispose: (entry, key) => {
        this.stats.entries--;
        this.stats.sizeBytes -= entry.size;
      }
    });
  }

  async get<T>(key: string): Promise<T | null> {
    const startTime = performance.now();
    const entry = this.cache.get(key);
    
    if (entry) {
      // Check if entry is still valid
      if (Date.now() - entry.timestamp <= entry.ttl) {
        entry.hits++;
        this.stats.hits++;
        this.updateHitRate();
        this.trackResponseTime(startTime);
        
        // Estimate cost savings (rough estimate: $0.002 per 1K tokens)
        this.stats.costSavings += this.estimateCostSaving(entry.data);
        
        return entry.data as T;
      } else {
        // Entry expired
        this.cache.delete(key);
      }
    }
    
    this.stats.misses++;
    this.updateHitRate();
    this.trackResponseTime(startTime);
    return null;
  }

  async set<T>(key: string, value: T, ttl?: number): Promise<void> {
    const size = this.calculateSize(value);
    const entry: CacheEntry<T> = {
      data: value,
      timestamp: Date.now(),
      ttl: ttl || this.options.ttl || 3600000,
      hits: 0,
      size
    };

    this.cache.set(key, entry);
    this.stats.entries++;
    this.stats.sizeBytes += size;
  }

  async delete(key: string): Promise<void> {
    this.cache.delete(key);
  }

  async clear(): Promise<void> {
    this.cache.clear();
    this.stats.entries = 0;
    this.stats.sizeBytes = 0;
  }

  getStats(): CacheStats {
    return { ...this.stats };
  }

  private updateHitRate(): void {
    const total = this.stats.hits + this.stats.misses;
    this.stats.hitRate = total > 0 ? (this.stats.hits / total) * 100 : 0;
  }

  private trackResponseTime(startTime: number): void {
    const responseTime = performance.now() - startTime;
    this.stats.avgResponseTime = 
      (this.stats.avgResponseTime + responseTime) / 2;
  }

  private calculateSize(value: any): number {
    // Rough estimation of object size in bytes
    return JSON.stringify(value).length * 2; // UTF-16
  }

  private estimateCostSaving(data: any): number {
    // Rough estimate based on typical AI response
    const tokens = JSON.stringify(data).length / 4;
    return (tokens / 1000) * 0.002; // $0.002 per 1K tokens
  }

  /**
   * Get cache warmth (how well utilized the cache is)
   */
  getCacheWarmth(): number {
    return (this.stats.entries / (this.options.maxEntries || 1000)) * 100;
  }
}
```

### Schritt 3: Redis Cache Implementation
Erstelle `packages/ai-adapter/src/cache/redis-cache.ts`:

```typescript
import { ICacheProvider, CacheStats } from './types';
import Redis from 'ioredis';

export class RedisCache implements ICacheProvider {
  private client: Redis;
  private stats: CacheStats = {
    hits: 0,
    misses: 0,
    entries: 0,
    sizeBytes: 0,
    hitRate: 0,
    avgResponseTime: 0,
    costSavings: 0
  };

  constructor(redisUrl?: string) {
    this.client = new Redis(redisUrl || process.env.REDIS_URL || 'redis://localhost:6379');
    
    this.client.on('error', (err) => {
      console.error('Redis Cache Error:', err);
    });
  }

  async get<T>(key: string): Promise<T | null> {
    const startTime = performance.now();
    
    try {
      const data = await this.client.get(key);
      
      if (data) {
        this.stats.hits++;
        const parsed = JSON.parse(data) as T;
        this.updateStats(startTime, true);
        return parsed;
      }
      
      this.stats.misses++;
      this.updateStats(startTime, false);
      return null;
    } catch (error) {
      console.error('Redis get error:', error);
      return null;
    }
  }

  async set<T>(key: string, value: T, ttl = 3600): Promise<void> {
    try {
      const serialized = JSON.stringify(value);
      
      if (ttl > 0) {
        await this.client.setex(key, ttl, serialized);
      } else {
        await this.client.set(key, serialized);
      }
      
      this.stats.entries++;
    } catch (error) {
      console.error('Redis set error:', error);
    }
  }

  async delete(key: string): Promise<void> {
    await this.client.del(key);
    this.stats.entries--;
  }

  async clear(): Promise<void> {
    await this.client.flushdb();
    this.stats.entries = 0;
  }

  getStats(): CacheStats {
    return { ...this.stats };
  }

  async disconnect(): Promise<void> {
    await this.client.quit();
  }

  private updateStats(startTime: number, hit: boolean): void {
    const responseTime = performance.now() - startTime;
    this.stats.avgResponseTime = 
      (this.stats.avgResponseTime + responseTime) / 2;
    
    const total = this.stats.hits + this.stats.misses;
    this.stats.hitRate = total > 0 ? (this.stats.hits / total) * 100 : 0;
    
    if (hit) {
      this.stats.costSavings += 0.002; // Rough estimate
    }
  }
}
```

### Schritt 4: Multi-Layer Cache Manager
Erstelle `packages/ai-adapter/src/cache/cache-manager.ts`:

```typescript
import { ICacheProvider, CacheOptions } from './types';
import { MemoryCache } from './memory-cache';
import { RedisCache } from './redis-cache';
import crypto from 'crypto';

export class CacheManager {
  private memoryCache: MemoryCache;
  private redisCache?: RedisCache;
  private keyPrefix = 'ai-adapter:';

  constructor(options: CacheOptions = {}) {
    this.memoryCache = new MemoryCache(options);
    
    if (options.enableRedis) {
      this.redisCache = new RedisCache(options.redisUrl);
    }
  }

  /**
   * Generate cache key from AI request options
   */
  generateKey(options: any): string {
    // Create deterministic key from request parameters
    const keyData = {
      model: options.model,
      messages: options.messages,
      temperature: options.temperature,
      maxTokens: options.maxTokens,
      // Exclude non-deterministic fields
      ...Object.keys(options)
        .filter(k => !['stream', 'signal', 'user'].includes(k))
        .reduce((acc, k) => ({ ...acc, [k]: options[k] }), {})
    };

    const hash = crypto
      .createHash('sha256')
      .update(JSON.stringify(keyData))
      .digest('hex')
      .substring(0, 16); // Use first 16 chars for brevity

    return `${this.keyPrefix}${options.model}:${hash}`;
  }

  /**
   * Get from cache (checks memory first, then Redis)
   */
  async get<T>(key: string): Promise<T | null> {
    // Try memory cache first (L1)
    const memoryResult = await this.memoryCache.get<T>(key);
    if (memoryResult !== null) {
      return memoryResult;
    }

    // Try Redis cache (L2)
    if (this.redisCache) {
      const redisResult = await this.redisCache.get<T>(key);
      
      if (redisResult !== null) {
        // Promote to memory cache
        await this.memoryCache.set(key, redisResult);
        return redisResult;
      }
    }

    return null;
  }

  /**
   * Set in both cache layers
   */
  async set<T>(key: string, value: T, ttl?: number): Promise<void> {
    // Set in memory cache
    await this.memoryCache.set(key, value, ttl);

    // Set in Redis cache
    if (this.redisCache) {
      await this.redisCache.set(key, value, ttl ? ttl / 1000 : 3600);
    }
  }

  /**
   * Invalidate cache entry
   */
  async invalidate(pattern: string): Promise<void> {
    // Clear from memory
    await this.memoryCache.clear(); // Simple implementation

    // Clear from Redis
    if (this.redisCache) {
      // In production, use SCAN to find and delete matching keys
      await this.redisCache.clear();
    }
  }

  /**
   * Get combined stats from all cache layers
   */
  getStats(): any {
    const memoryStats = this.memoryCache.getStats();
    const redisStats = this.redisCache?.getStats();

    return {
      memory: memoryStats,
      redis: redisStats,
      combined: {
        totalHits: memoryStats.hits + (redisStats?.hits || 0),
        totalMisses: memoryStats.misses + (redisStats?.misses || 0),
        totalCostSavings: memoryStats.costSavings + (redisStats?.costSavings || 0),
        overallHitRate: this.calculateOverallHitRate(memoryStats, redisStats)
      }
    };
  }

  private calculateOverallHitRate(memory: any, redis: any): number {
    const totalHits = memory.hits + (redis?.hits || 0);
    const totalRequests = totalHits + memory.misses + (redis?.misses || 0);
    return totalRequests > 0 ? (totalHits / totalRequests) * 100 : 0;
  }

  /**
   * Warm up cache with common queries
   */
  async warmUp(commonQueries: any[]): Promise<void> {
    console.log(`Warming up cache with ${commonQueries.length} queries...`);
    
    for (const query of commonQueries) {
      const key = this.generateKey(query);
      // Simulate cached response
      await this.set(key, query.expectedResponse, 7200000); // 2 hours
    }
  }

  /**
   * Clean up resources
   */
  async destroy(): Promise<void> {
    await this.memoryCache.clear();
    
    if (this.redisCache) {
      await this.redisCache.disconnect();
    }
  }
}
```

### Schritt 5: Integration in AI Adapter
Update `packages/ai-adapter/src/ai-adapter.ts`:

```typescript
import { CacheManager } from './cache/cache-manager';

export class AIAdapter {
  private cacheManager: CacheManager;
  private cacheEnabled = true;

  constructor(configs: AdapterConfig[]) {
    // Initialize cache
    this.cacheManager = new CacheManager({
      maxEntries: 1000,
      maxSize: 100, // 100MB
      ttl: 3600000, // 1 hour
      enableRedis: process.env.ENABLE_REDIS_CACHE === 'true',
      redisUrl: process.env.REDIS_URL
    });

    // ... rest of initialization
  }

  async complete(options: AICompletionOptions): Promise<AICompletion> {
    // Check cache first
    if (this.cacheEnabled && !options.noCache) {
      const cacheKey = this.cacheManager.generateKey(options);
      const cached = await this.cacheManager.get<AICompletion>(cacheKey);
      
      if (cached) {
        console.log(`Cache hit for model ${options.model}`);
        return {
          ...cached,
          cached: true // Mark as cached response
        };
      }
    }

    // Make actual API call
    const provider = this.getProvider(options.provider);
    const response = await provider.complete(options);

    // Cache the response
    if (this.cacheEnabled && !options.noCache) {
      const cacheKey = this.cacheManager.generateKey(options);
      const ttl = this.calculateTTL(options);
      await this.cacheManager.set(cacheKey, response, ttl);
    }

    return response;
  }

  private calculateTTL(options: AICompletionOptions): number {
    // Dynamic TTL based on content type
    if (options.messages.some(m => m.content.includes('current'))) {
      return 300000; // 5 minutes for time-sensitive content
    }
    
    if (options.temperature > 0.7) {
      return 1800000; // 30 minutes for creative content
    }
    
    return 3600000; // 1 hour default
  }

  /**
   * Get cache statistics
   */
  getCacheStats(): any {
    return this.cacheManager.getStats();
  }

  /**
   * Clear cache
   */
  async clearCache(pattern?: string): Promise<void> {
    if (pattern) {
      await this.cacheManager.invalidate(pattern);
    } else {
      await this.cacheManager.invalidate('*');
    }
  }

  /**
   * Toggle cache on/off
   */
  setCacheEnabled(enabled: boolean): void {
    this.cacheEnabled = enabled;
  }
}
```

### Schritt 6: Tests
Erstelle `packages/ai-adapter/src/cache/__tests__/caching.test.ts`:

```typescript
import { CacheManager } from '../cache-manager';
import { MemoryCache } from '../memory-cache';

describe('Response Caching', () => {
  let cacheManager: CacheManager;

  beforeEach(() => {
    cacheManager = new CacheManager({
      maxEntries: 10,
      maxSize: 1,
      ttl: 1000
    });
  });

  afterEach(async () => {
    await cacheManager.destroy();
  });

  test('should generate consistent cache keys', () => {
    const options = {
      model: 'gpt-4',
      messages: [{ role: 'user', content: 'Hello' }],
      temperature: 0.7
    };

    const key1 = cacheManager.generateKey(options);
    const key2 = cacheManager.generateKey(options);

    expect(key1).toBe(key2);
  });

  test('should cache and retrieve responses', async () => {
    const key = 'test-key';
    const value = { content: 'Test response', usage: { tokens: 100 } };

    await cacheManager.set(key, value);
    const retrieved = await cacheManager.get(key);

    expect(retrieved).toEqual(value);
  });

  test('should respect TTL', async () => {
    const key = 'ttl-test';
    const value = { content: 'Expires soon' };

    await cacheManager.set(key, value, 100); // 100ms TTL
    
    // Should exist immediately
    expect(await cacheManager.get(key)).toEqual(value);

    // Should expire after TTL
    await new Promise(resolve => setTimeout(resolve, 150));
    expect(await cacheManager.get(key)).toBeNull();
  });

  test('should track cache statistics', async () => {
    const key = 'stats-test';
    const value = { content: 'Stats test' };

    // Miss
    await cacheManager.get(key);
    
    // Set
    await cacheManager.set(key, value);
    
    // Hit
    await cacheManager.get(key);
    await cacheManager.get(key);

    const stats = cacheManager.getStats();
    
    expect(stats.memory.hits).toBe(2);
    expect(stats.memory.misses).toBe(1);
    expect(stats.memory.hitRate).toBeGreaterThan(60);
  });

  test('should handle LRU eviction', async () => {
    const smallCache = new MemoryCache({
      maxEntries: 3,
      ttl: 60000
    });

    // Fill cache
    await smallCache.set('key1', 'value1');
    await smallCache.set('key2', 'value2');
    await smallCache.set('key3', 'value3');

    // Access key1 to make it recently used
    await smallCache.get('key1');

    // Add new item, should evict key2 (least recently used)
    await smallCache.set('key4', 'value4');

    expect(await smallCache.get('key1')).toBe('value1');
    expect(await smallCache.get('key2')).toBeNull(); // Evicted
    expect(await smallCache.get('key3')).toBe('value3');
    expect(await smallCache.get('key4')).toBe('value4');
  });
});
```

## Verifizierung

### Test 1: Cache Hit Rate
```bash
npm test -- cache-hit-rate
# Should show > 40% hit rate for repeated queries
```

### Test 2: Cost Savings
```typescript
// Monitor cost savings
const stats = aiAdapter.getCacheStats();
console.log(`Estimated savings: $${stats.combined.totalCostSavings.toFixed(2)}`);
```

### Test 3: Performance Impact
```bash
# Benchmark with and without cache
npm run benchmark -- --cache=on
npm run benchmark -- --cache=off
```

## Erfolgskriterien
- [ ] Memory Cache mit LRU implementiert
- [ ] Redis Cache optional verfügbar
- [ ] Multi-Layer Cache funktioniert
- [ ] Cache Keys deterministisch
- [ ] TTL Management funktioniert
- [ ] Hit Rate > 40% bei typischer Nutzung
- [ ] Cost Savings messbar
- [ ] Keine Memory Leaks

## Potentielle Probleme

### Problem: Cache Invalidierung
**Lösung**: Versioned Cache Keys
```typescript
const key = `v2:${this.generateKey(options)}`;
```

### Problem: Cache Stampede
**Lösung**: Lock-based Generation
```typescript
if (isGenerating(key)) {
  await waitForGeneration(key);
  return this.get(key);
}
```

### Problem: Memory Overflow
**Lösung**: Aggressive Eviction
```typescript
if (memoryUsage() > threshold) {
  await this.evictOldestEntries(0.2); // Evict 20%
}
```

## Zeitschätzung
- Memory Cache Implementation: 6 Minuten
- Redis Cache Implementation: 5 Minuten
- Cache Manager: 4 Minuten
- Integration: 3 Minuten
- Tests: 4 Minuten
- **Total: 22 Minuten**

## Nächster Schritt
Nach erfolgreicher Cache Implementation → [12.11.3 Token Optimization](./12.11.3-token-optimization.md)