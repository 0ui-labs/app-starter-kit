# 11.05 Streaming Support

## Ziel
Implementierung einer einheitlichen Streaming-Schnittstelle für alle AI Provider mit Server-Sent Events (SSE) und WebSocket Support.

## Umfang
- Unified streaming interface
- Server-Sent Events implementation
- Stream transformation utilities
- Backpressure handling
- Error recovery in streams

## Tasks

### 1. Streaming Types & Interfaces
```typescript
// packages/ai-adapter/src/streaming/types.ts
export interface StreamChunk {
  id: string;
  type: 'content' | 'tool_call' | 'error' | 'done';
  content?: string;
  delta?: string;
  toolCall?: ToolCallChunk;
  error?: StreamError;
  usage?: TokenUsage;
  finishReason?: string;
}

export interface StreamOptions {
  onChunk?: (chunk: StreamChunk) => void;
  onError?: (error: StreamError) => void;
  onComplete?: (response: CompletionResponse) => void;
  signal?: AbortSignal;
  backpressureStrategy?: 'buffer' | 'drop' | 'block';
}

export interface StreamAdapter<T> {
  transform(chunk: T): StreamChunk;
  accumulate(chunks: StreamChunk[]): CompletionResponse;
}
```

### 2. Stream Manager Implementation
```typescript
// packages/ai-adapter/src/streaming/stream-manager.ts
export class StreamManager {
  private activeStreams = new Map<string, AbortController>();
  
  async *processStream<T>(
    stream: AsyncIterable<T>,
    adapter: StreamAdapter<T>,
    options?: StreamOptions
  ): AsyncIterableIterator<StreamChunk> {
    const streamId = generateStreamId();
    const controller = new AbortController();
    this.activeStreams.set(streamId, controller);
    
    const chunks: StreamChunk[] = [];
    
    try {
      for await (const rawChunk of stream) {
        // Check for abort
        if (controller.signal.aborted) {
          break;
        }
        
        const chunk = adapter.transform(rawChunk);
        chunks.push(chunk);
        
        // Call callback if provided
        options?.onChunk?.(chunk);
        
        // Handle backpressure
        await this.handleBackpressure(options?.backpressureStrategy);
        
        yield chunk;
      }
      
      // Send completion
      const response = adapter.accumulate(chunks);
      options?.onComplete?.(response);
      
      yield {
        id: streamId,
        type: 'done',
        usage: response.usage,
        finishReason: response.finishReason,
      };
    } catch (error) {
      const streamError = this.wrapError(error);
      options?.onError?.(streamError);
      
      yield {
        id: streamId,
        type: 'error',
        error: streamError,
      };
    } finally {
      this.activeStreams.delete(streamId);
    }
  }
  
  abort(streamId: string): void {
    const controller = this.activeStreams.get(streamId);
    controller?.abort();
  }
  
  abortAll(): void {
    for (const controller of this.activeStreams.values()) {
      controller.abort();
    }
    this.activeStreams.clear();
  }
}
```

### 3. Provider-Specific Stream Adapters
```typescript
// packages/ai-adapter/src/streaming/adapters/openai-adapter.ts
export class OpenAIStreamAdapter implements StreamAdapter<OpenAI.ChatCompletionChunk> {
  transform(chunk: OpenAI.ChatCompletionChunk): StreamChunk {
    const choice = chunk.choices[0];
    
    return {
      id: chunk.id,
      type: choice.delta.tool_calls ? 'tool_call' : 'content',
      delta: choice.delta.content || undefined,
      toolCall: this.transformToolCall(choice.delta.tool_calls),
      finishReason: choice.finish_reason,
    };
  }
  
  accumulate(chunks: StreamChunk[]): CompletionResponse {
    let content = '';
    const toolCalls: ToolCall[] = [];
    
    for (const chunk of chunks) {
      if (chunk.delta) {
        content += chunk.delta;
      }
      if (chunk.toolCall) {
        this.mergeToolCall(toolCalls, chunk.toolCall);
      }
    }
    
    return {
      content,
      toolCalls: toolCalls.length > 0 ? toolCalls : undefined,
      usage: chunks[chunks.length - 1]?.usage,
      finishReason: chunks[chunks.length - 1]?.finishReason,
    };
  }
}
```

### 4. Server-Sent Events (SSE) Support
```typescript
// packages/ai-adapter/src/streaming/sse.ts
export class SSEStreamWriter {
  constructor(private writer: WritableStreamDefaultWriter) {}
  
  async writeChunk(chunk: StreamChunk): Promise<void> {
    const sseMessage = this.formatSSE(chunk);
    const encoder = new TextEncoder();
    await this.writer.write(encoder.encode(sseMessage));
  }
  
  private formatSSE(chunk: StreamChunk): string {
    const data = JSON.stringify(chunk);
    return `data: ${data}\n\n`;
  }
  
  async writeKeepAlive(): Promise<void> {
    await this.writer.write(new TextEncoder().encode(':keepalive\n\n'));
  }
  
  async close(): Promise<void> {
    await this.writer.write(new TextEncoder().encode('data: [DONE]\n\n'));
    await this.writer.close();
  }
}

// Next.js Route Handler Example
export async function POST(request: Request) {
  const stream = new TransformStream();
  const writer = new SSEStreamWriter(stream.writable.getWriter());
  
  // Process in background
  (async () => {
    try {
      const aiStream = await aiAdapter.stream(request);
      
      for await (const chunk of aiStream) {
        await writer.writeChunk(chunk);
      }
    } catch (error) {
      await writer.writeChunk({
        id: 'error',
        type: 'error',
        error: { message: error.message },
      });
    } finally {
      await writer.close();
    }
  })();
  
  return new Response(stream.readable, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}
```

### 5. Stream Utilities
```typescript
// packages/ai-adapter/src/streaming/utils.ts
export class StreamUtils {
  // Buffer stream chunks for batch processing
  static async *buffer<T>(
    stream: AsyncIterable<T>,
    size: number,
    timeout?: number
  ): AsyncIterableIterator<T[]> {
    const buffer: T[] = [];
    let timer: NodeJS.Timeout;
    
    for await (const chunk of stream) {
      buffer.push(chunk);
      
      if (buffer.length >= size) {
        yield [...buffer];
        buffer.length = 0;
        clearTimeout(timer);
      } else if (timeout) {
        clearTimeout(timer);
        timer = setTimeout(() => {
          if (buffer.length > 0) {
            yield [...buffer];
            buffer.length = 0;
          }
        }, timeout);
      }
    }
    
    if (buffer.length > 0) {
      yield buffer;
    }
  }
  
  // Transform stream with error handling
  static async *mapStream<T, U>(
    stream: AsyncIterable<T>,
    transform: (item: T) => U | Promise<U>,
    onError?: (error: Error, item: T) => U | undefined
  ): AsyncIterableIterator<U> {
    for await (const item of stream) {
      try {
        yield await transform(item);
      } catch (error) {
        if (onError) {
          const fallback = onError(error as Error, item);
          if (fallback !== undefined) {
            yield fallback;
          }
        } else {
          throw error;
        }
      }
    }
  }
  
  // Merge multiple streams
  static async *merge<T>(
    ...streams: AsyncIterable<T>[]
  ): AsyncIterableIterator<T> {
    const iterators = streams.map(s => s[Symbol.asyncIterator]());
    const promises = iterators.map((it, idx) => 
      it.next().then(result => ({ idx, result }))
    );
    
    while (promises.length > 0) {
      const { idx, result } = await Promise.race(promises);
      
      if (!result.done) {
        yield result.value;
        promises[idx] = iterators[idx]
          .next()
          .then(result => ({ idx, result }));
      } else {
        promises.splice(idx, 1);
        iterators.splice(idx, 1);
      }
    }
  }
}
```

### 6. React Hook for Streaming
```typescript
// packages/ai-adapter/src/streaming/react-hook.ts
export function useAIStream(endpoint: string) {
  const [messages, setMessages] = useState<StreamChunk[]>([]);
  const [isStreaming, setIsStreaming] = useState(false);
  const [error, setError] = useState<Error | null>(null);
  const abortRef = useRef<AbortController>();
  
  const startStream = useCallback(async (prompt: string) => {
    setIsStreaming(true);
    setError(null);
    setMessages([]);
    
    abortRef.current = new AbortController();
    
    try {
      const response = await fetch(endpoint, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ prompt }),
        signal: abortRef.current.signal,
      });
      
      const reader = response.body?.getReader();
      const decoder = new TextDecoder();
      
      while (reader) {
        const { done, value } = await reader.read();
        if (done) break;
        
        const text = decoder.decode(value);
        const lines = text.split('\n');
        
        for (const line of lines) {
          if (line.startsWith('data: ')) {
            const data = line.slice(6);
            if (data === '[DONE]') break;
            
            const chunk = JSON.parse(data) as StreamChunk;
            setMessages(prev => [...prev, chunk]);
          }
        }
      }
    } catch (err) {
      if (err.name !== 'AbortError') {
        setError(err as Error);
      }
    } finally {
      setIsStreaming(false);
    }
  }, [endpoint]);
  
  const abort = useCallback(() => {
    abortRef.current?.abort();
  }, []);
  
  return { messages, isStreaming, error, startStream, abort };
}
```

## Dateien zu erstellen/ändern
- `packages/ai-adapter/src/streaming/types.ts` - Streaming interfaces
- `packages/ai-adapter/src/streaming/stream-manager.ts` - Core streaming logic
- `packages/ai-adapter/src/streaming/adapters/*.ts` - Provider adapters
- `packages/ai-adapter/src/streaming/sse.ts` - SSE implementation
- `packages/ai-adapter/src/streaming/utils.ts` - Stream utilities
- `packages/ai-adapter/src/streaming/react-hook.ts` - React integration

## Akzeptanzkriterien
- [ ] Einheitliche streaming interface für alle Provider
- [ ] SSE implementation funktioniert
- [ ] Stream adapters für alle Provider
- [ ] Backpressure handling implementiert
- [ ] Abort functionality funktioniert
- [ ] React hook für client-side streaming
- [ ] Error recovery in streams
- [ ] Tests für streaming logic

## Abhängigkeiten
- Tasks 11.01-11.04 (Provider implementations)
- Next.js App Router für SSE support
- React 18+ für streaming hooks

## Geschätzter Aufwand
75-90 Minuten